{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning VGG 16 and VGG 19 using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download the dataset from the below url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries as shown below\n",
    "\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-size all the images to this\n",
    "IMAGE_SIZE = [224, 224]\n",
    "\n",
    "train_path = 'Datasets/train'\n",
    "valid_path = 'Datasets/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 31s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG\n",
    "# Here we will be using imagenet weights\n",
    "\n",
    "resnet = ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't train existing weights\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # useful for getting number of output classes\n",
    "folders = glob('C:/Users/shiva\\Documents/MachineLearningProjects/Face_Recognition_Deep_Learning/Dataset/Train*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our layers - you can add more if you want\n",
    "x = Flatten()(resnet.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(len(folders), activation='softmax')(x)\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs=resnet.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100352)       0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            100353      flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,688,065\n",
      "Trainable params: 100,353\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# view the structure of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the model what cost and optimization method to use\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Image Data Generator to import the images from the dataset\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Make sure you provide the same target size as initialied for the image size\n",
    "training_set = train_datagen.flow_from_directory('C:/Users/shiva/Documents/MachineLearningProjects/Face_Recognition_Deep_Learning/Dataset/Train',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('C:/Users/shiva/Documents/MachineLearningProjects/Face_Recognition_Deep_Learning/Dataset/Test',\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 7 steps, validate for 2 steps\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 57s 8s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 46s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 48s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 46s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 47s 7s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 43s 6s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 28s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 27s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 27s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 27s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 27s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 27s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 27s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 28s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 30s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 29s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 29s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 28s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 32s 5s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 29s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 28s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 30s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 29s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 28s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 29s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 29s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 28s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 30s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 29s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 28s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 28s 4s/step - loss: 7.8033 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "# Run the cell. It will take some time to execute\n",
    "r = model.fit_generator(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=50,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss\n",
    "plt.plot(r.history['loss'], label='train loss')\n",
    "plt.plot(r.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('LossVal_loss')\n",
    "\n",
    "# plot the accuracy\n",
    "plt.plot(r.history['accuracy'], label='train acc')\n",
    "plt.plot(r.history['val_accuracy'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('AccVal_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it as a h5 file\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model.save('model_resnet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('model_resnet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 6.7060997e+01,  5.4221001e+01,  4.7320000e+01],\n",
       "         [ 6.9060997e+01,  5.6221001e+01,  4.9320000e+01],\n",
       "         [ 7.3060997e+01,  6.0221001e+01,  5.3320000e+01],\n",
       "         ...,\n",
       "         [ 7.4060997e+01,  5.6221001e+01,  4.6320000e+01],\n",
       "         [ 5.5060997e+01,  3.7221001e+01,  2.7320000e+01],\n",
       "         [ 4.1060997e+01,  2.3221001e+01,  1.3320000e+01]],\n",
       "\n",
       "        [[ 7.5060997e+01,  6.2221001e+01,  5.5320000e+01],\n",
       "         [ 7.8060997e+01,  6.5221001e+01,  5.8320000e+01],\n",
       "         [ 8.1060997e+01,  6.8221001e+01,  6.1320000e+01],\n",
       "         ...,\n",
       "         [ 9.7060997e+01,  7.9221001e+01,  6.9320000e+01],\n",
       "         [ 7.3060997e+01,  5.5221001e+01,  4.5320000e+01],\n",
       "         [ 4.9060997e+01,  3.1221001e+01,  2.1320000e+01]],\n",
       "\n",
       "        [[ 8.7060997e+01,  7.4221001e+01,  6.7320000e+01],\n",
       "         [ 9.0060997e+01,  7.7221001e+01,  7.0320000e+01],\n",
       "         [ 9.3060997e+01,  8.0221001e+01,  7.3320000e+01],\n",
       "         ...,\n",
       "         [ 1.0106100e+02,  8.3221001e+01,  7.3320000e+01],\n",
       "         [ 7.5060997e+01,  5.7221001e+01,  4.7320000e+01],\n",
       "         [ 5.0060997e+01,  3.2221001e+01,  2.2320000e+01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.0406100e+02,  8.9221001e+01,  9.4320000e+01],\n",
       "         [ 1.0206100e+02,  8.7221001e+01,  9.2320000e+01],\n",
       "         [ 9.9060997e+01,  8.4221001e+01,  8.9320000e+01],\n",
       "         ...,\n",
       "         [-1.0939003e+01, -1.6778999e+01, -1.4680000e+01],\n",
       "         [-1.0939003e+01, -1.6778999e+01, -1.4680000e+01],\n",
       "         [-1.0939003e+01, -1.6778999e+01, -1.4680000e+01]],\n",
       "\n",
       "        [[ 1.0606100e+02,  9.1221001e+01,  9.6320000e+01],\n",
       "         [ 1.0406100e+02,  8.9221001e+01,  9.4320000e+01],\n",
       "         [ 1.0006100e+02,  8.5221001e+01,  9.0320000e+01],\n",
       "         ...,\n",
       "         [-5.9390030e+00, -1.1778999e+01, -9.6800003e+00],\n",
       "         [-5.9390030e+00, -1.1778999e+01, -9.6800003e+00],\n",
       "         [-5.9390030e+00, -1.1778999e+01, -9.6800003e+00]],\n",
       "\n",
       "        [[ 1.0806100e+02,  9.4221001e+01,  9.6320000e+01],\n",
       "         [ 1.0606100e+02,  9.2221001e+01,  9.4320000e+01],\n",
       "         [ 1.0206100e+02,  8.8221001e+01,  9.0320000e+01],\n",
       "         ...,\n",
       "         [ 6.0997009e-02, -5.7789993e+00, -3.6800003e+00],\n",
       "         [ 6.0997009e-02, -5.7789993e+00, -3.6800003e+00],\n",
       "         [ 6.0997009e-02, -5.7789993e+00, -3.6800003e+00]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=image.load_img('Datasets/Test/lamborghini/11.jpg',target_size=(224,224))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[252., 252., 252.],\n",
       "        [252., 252., 252.],\n",
       "        [252., 252., 252.],\n",
       "        ...,\n",
       "        [194., 188., 174.],\n",
       "        [215., 209., 197.],\n",
       "        [241., 235., 223.]],\n",
       "\n",
       "       [[252., 252., 252.],\n",
       "        [252., 252., 252.],\n",
       "        [252., 252., 252.],\n",
       "        ...,\n",
       "        [247., 245., 233.],\n",
       "        [245., 242., 233.],\n",
       "        [244., 241., 232.]],\n",
       "\n",
       "       [[252., 252., 252.],\n",
       "        [252., 252., 252.],\n",
       "        [252., 252., 252.],\n",
       "        ...,\n",
       "        [245., 248., 241.],\n",
       "        [244., 250., 248.],\n",
       "        [244., 250., 248.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[190., 206., 229.],\n",
       "        [190., 206., 229.],\n",
       "        [190., 206., 229.],\n",
       "        ...,\n",
       "        [172., 179., 187.],\n",
       "        [171., 180., 187.],\n",
       "        [171., 180., 187.]],\n",
       "\n",
       "       [[187., 205., 227.],\n",
       "        [187., 205., 227.],\n",
       "        [187., 205., 227.],\n",
       "        ...,\n",
       "        [172., 179., 187.],\n",
       "        [171., 180., 187.],\n",
       "        [171., 180., 187.]],\n",
       "\n",
       "       [[185., 206., 227.],\n",
       "        [185., 206., 227.],\n",
       "        [187., 205., 227.],\n",
       "        ...,\n",
       "        [172., 179., 187.],\n",
       "        [171., 180., 187.],\n",
       "        [171., 180., 187.]]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=image.img_to_array(img)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.expand_dims(x,axis=0)\n",
    "img_data=preprocess_input(x)\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02122196, 0.49325418, 0.4855238 ]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.argmax(model.predict(img_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
